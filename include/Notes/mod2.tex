\section{Probability Theory}\label{mod2:section:ProbabilityTheory}


\begin{axiom}  \label{mod2:axiom:ProbabilityAxioms}
	Consider an experiment whose sample space is $S$. The \textbf{probability of the event $E$}, denoted as $P(E)$, is a number that satisfies the following three axioms	
	\begin{enumerate}[label = \bfseries \arabic*]
		\item 	$0 \leq P(E) \leq 1 , $ \label{mod2:axiom:ProbabilityAxiom:1}
				
			
		
		\item 	$	P(S) = 1, $ \label{mod2:axiom:ProbabilityAxiom:2}
				
			
		
		\item 	For any sequence of mutually exclusive events $E_1, E_2, ... , E_n$.
		
			
					$P\left( \cup_{i=1}^n E_i \right) = \sum_{i=1}^n P(E_i), $ \label{mod2:axiom:ProbabilityAxiom:3}
			
			
			
				Eg. $P(E_1 \cup E_2) = P(E_1)+P(E_2)$.
		
		
	\end{enumerate}
\end{axiom}


\begin{defn} \label{mod2:defn:Probability}
	Furthermore, we define,
	\begin{equation}
		P(\text{event E occurs}) = \frac{\text{no. of times A can occur}}{\text{total number of outcomes}}. 
	\end{equation}
\end{defn}



\begin{defn} \label{mod2:defn:Complement}
	The probability of the \textbf{complement} of event $A$, denoted as $P(A^c)$ or $P(\bar{A})$ or $P(A^\prime)$, is defined as
	\begin{equation}
		P(\bar{A}) = 1 - P(A).
	\end{equation}
\end{defn}


\begin{prop} \label{mod2:prop:SetPropositions}
Some useful propositions can be easily derived using the above and drawing your own diagrams
\begin{align}
P(A \cup B) &= P(A) + P(B) - P(A \cap B), \label{mod2:eq:SetProp:1}  \\ 
P(A^c \cap B) &= P(B) - P(A \cap B), \label{mod2:eq:SetProp:2} \\ 
P(A^c \cap B^c) &=  P(A \cup B)^c ,\label{mod2:eq:SetProp:3} \\ 
P(A^c \cup B^c) &= P (A \cap B) ^c .\label{mod2:eq:SetProp:4} \ 
\end{align}
\end{prop}


\begin{defn} \label{mod2:defn:MutuallyExclusive}
Two events $A$ and $B$ are said to be \textbf{mutually exclusive} if and only if
\begin{equation}	
	P ( A \cap B) = 0. \label{mod2:eq:MutuallyExclusive:1} \ 
\end{equation}
Applying Eq~[\ref{mod2:eq:SetProp:1}], this implies 
\begin{equation}
	P(A \cup B) = P(A) + P(B).\  \label{mod2:eq:MutuallyExclusive:2}
\end{equation}
\end{defn}


\begin{defn} \label{mod2:defn:Independent}
Two events $A$ and $B$ are said to be \textbf{independent} if and only if
\begin{equation}
P(A \cap B) = P (A) \times P(B). \label{mod2:eq:Independent} \ 
\end{equation}
\end{defn}

\begin{defn} \label{mod2:defn:Conditional}
For events $A$ and $B$, the \textbf{conditional probability} of $A$ given $B$ has occurred, denoted by $P(A|B)$, is defined by
\begin{equation}
P(A|B) = \frac{P(A\cap B)}{P(B)}. \label{mod2:eq:ConditionalProbability} \ 
\end{equation}
\end{defn}






\section{Discrete Random Variables} \label{mod2:section:DiscreteRandomVariables}


\begin{defn} \label{mod2:defn:DiscreteRandomVar}
	A \textbf{discrete random variable}, $X$, can take on at most a countable number of possible values. We define its \textit{probability mass} function, $P(x)$ by
	\begin{equation}
	P(x) =P(X = x).
	\end{equation}
\end{defn}

\begin{prop}\label{mod2:prop:Discrete:Properties}
	If $X$ is a discrete random variable with probability mass function $P(x)$, then we know from Axiom~[\ref{mod2:axiom:ProbabilityAxiom:1}] that
	\begin{equation}
	0 \leq P(X=x) \leq 1 \: \forall x.
	\end{equation}
	and from Axiom~[\ref{mod2:axiom:ProbabilityAxiom:2}] and Axiom~[\ref{mod2:axiom:ProbabilityAxiom:3}] that
	\begin{equation}
	\sum_{\forall x}P(X=x) = 1 .
	\end{equation}
\end{prop}

\begin{defn} \label{mod2:defn:Discrete:Expectation}
	If $X$ is a discrete random variable, the \textbf{expectation value}, \textbf{mean} or \textbf{first moment of $X$}, $E[X]$, is defined by
	\begin{equation}
	E[X]= \sum_{\forall i} x_i P(X=x_i). \label{mod2:eq:Discrete:Expectation}\ 
	\end{equation}
\end{defn}

\begin{defn} \label{mod2:defn:Discrete:SecondMoment}
	If $X$ is a discrete random variable, the \textbf{second moment of $X$} is defined by
	\begin{equation}
	E[X^2] = \sum_{\forall i} x_i^2 P(X=x_i). \label{mod2:eq:Discrete:SecondMoment} \ 
	\end{equation}
\end{defn}

\begin{defn}\label{mod2:defn:Discrete:Variance}
	If $X$ is a random variable with mean $\mu$, then the \textbf{variance}, $Var[X]$, is defined by
	\begin{align}
	Var[X] &= E[(X-\mu)^2],  \label{mod2:eq:Discrete:Variance:1}  \\
	&= E[X^2] - (E[X])^2  = E[X^2] - \mu^2 . \label{mod2:eq:Discrete:Variance:2} \
	\end{align}
\end{defn}



\section{Discrete Uniform Distribution} \label{mo2:section:DiscreteUniform}

\begin{defn} \label{mod2:defn:DiscreteUniform}
	A discrete random variable $X$ is said to follow a \textbf{discrete uniform distribution} if $X$ can take a finite number of values which are observed with equal probabilities. Therefore, the probability mass function, $P$, of $X$ which can take $n$ possible values,  is given by
	\begin{equation}
		P(X = x_i) = \frac{1}{n} \label{mod2:eq:DiscreteUniform}.
	\end{equation}
\end{defn}

\begin{note}\label{mod2:note:DiscreteUniform}
	In the discrete case, the notation $X \sim \text{Unif}\{a, b\}$ or $X \sim \text{U}\{a, b\}$ implies that $X$ can take \textbf{integer} values $x$ such that $a \leq x \leq b$. Here, the total numer of values $X$ can take is given by
	\begin{equation}
		n = b - a + 1,
	\end{equation}
	Therefore the p.m.f of $X$ can be given by
	\begin{equation}
	P(X = x)= \frac{1}{b - a + 1}\ \quad a \leq x \leq b.
	\end{equation}
		The \textit{expected value (mean)} and \textit{variance} of $X \sim \text{U}\{a,b\}$ are given by:
	\begin{align}
	E[X] &= \frac{a+b}{2}, \label{mod2:eq:DiscreteUniform:Mean} \\
	Var[X] &= \frac{(b-a+1)^2 - 1}{12}. \label{mod2:eq:DiscreteUniform:Variance} 
	\end{align}
\end{note}




\section{Binomial Distribution} \label{mod2:section:Binomial}
\begin{defn} \label{mod2:defn:Binomial}
	A discrete random variable $X$ is said to follow a \textbf{binomial distribution} with parameters $n$ and $p$, $X \sim \text{Bin}(n,p)$, if its probability mass function, $P$, is given by
	\begin{equation}
	P(X = x) = { n \choose x} p^x (1-p)^{n-x} \quad x \in ( 0, 1, 2, ... , n). \label{mod2:eq:BinomialDist} \ 
	\end{equation} 
\end{defn}

\begin{note}
	The \textit{expected value (mean)} and \textit{variance} of $X$ are given by:
	\begin{align}
	E[X] &= np,  \label{mod2:eq:Binomial:Mean} \\ 
	Var[X] &= np(1-p), \label{mod2:eq:Binomial:Variance} \\
	&= npq  \: \text{ where } q = (1-p).\ 
	\end{align}
\end{note}

\begin{note} \label{mod2:note:Binomial:Conditions}
	There are four conditions that describe a binomial distribution
	\begin{enumerate}[label = (\roman*)]
		\item The experiment consist of a fixed number of trials, $n$.
		\item The trials are independent.
		\item Each trial can be classified as a success or failure.
		\item The probability of success, $p$, is constant
	\end{enumerate}
\end{note}



\section{Geometric Distribution} \label{mod2:section:Geometric}
\begin{defn}\label{mod2:defn:Geomtric}
	A discrete random variable $X$ is said to follow a \textbf{geomteric distribution} with parameter $p$, $X \sim \text{Geo}(p)$, if its probability mass function, $P$, is given by
\begin{equation}
P(X = x) = p(1-p)^{x-1} \quad x \in (1, 2, ...). \label{mod2:eq:GeometricDist} \ 
\end{equation} 
\end{defn}

\begin{note}
	The \textit{expected value (mean)} and \textit{variance} of $X$ are given by:
	\begin{align}
	E[X] &= \frac{1}{p},  \label{mod2:eq:Geometric:Mean} \\ 
	Var[X] &= \frac{1-p}{p^2}, \label{mod2:eq:Geomtric:Variance} \\
	&= \frac{q}{p^2}  \: \text{ where } q = (1-p).\ 
	\end{align}
\end{note}

\begin{note} \label{mod2:note:Geometric:Conditions}
	There are three conditions that describe a geomtric distribution
	\begin{enumerate}[label = (\roman*)]
		\item The trials are independent.
		\item Each trial can be classified as a success or failure.
		\item The probability of success, $p$, is constant
	\end{enumerate}
\end{note}



\section{Poisson Distribution}\label{mod2:section:Poisson}
\begin{defn} \label{mod2:defn:Poisson}
	A discrete random variable $X$ is said to follow a \textbf{poisson distribution} with parameter $\lambda$, $X \sim \text{Pois}(\lambda)$, if its probability mass function, $P$, is given by
\begin{equation}
P(X = x) =\frac{ \lambda ^ x e^\lambda}{x!} \quad x \in (0, 1, 2, ...). \label{mod2:eq:PoissonDist} \ 
\end{equation} 
\end{defn}


\begin{note}
	The \textit{expected value (mean)} and \textit{variance} of $X$ are given by:
	\begin{align}
	E[X] &= \lambda,  \label{mod2:eq:Poisson:Mean} \\ 
	Var[X] &= \lambda. \label{mod2:eq:Poisson:Variance} 
	\end{align}
\end{note}


\begin{note} \label{mod2:note:Poisson:Conditions}
	The Poisson distribution is popular for modeling the number of times an event occurs in an interval of time. There are two conditions that describe a poisson distribution
	\begin{enumerate}[label = (\roman*)]
		\item Events are independent of each other.
		\item The average rate at which events occur, $\lambda$, is independent of any occurrences.
	\end{enumerate}
\end{note}









